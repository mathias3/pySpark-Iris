{"nbformat_minor": 1, "cells": [{"execution_count": 1, "cell_type": "code", "source": "from pyspark.sql.types import *\nfrom pyspark.ml.feature import Bucketizer\nsplits = [-float(\"inf\"), 8.0, 12.0, 15.0, float(\"inf\")]\ntemp = [(1, 10.2), (2, 17.1), (3, 9.6), (4, 5.0), (5, 3.4)]\nrainfall = sc.parallelize(temp)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1481411712782_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-mysia.05ajb0c3rupuvg3di2sqgf1xuf.cx.internal.cloudapp.net:8088/proxy/application_1481411712782_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.5:30060/node/containerlogs/container_1481411712782_0007_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkContext available as 'sc'.\nHiveContext available as 'sqlContext'.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "fields = [StructField(\"id\", IntegerType(), True), StructField(\"rainfall\", DoubleType(), True)]\nschema = StructType(fields)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 3, "cell_type": "code", "source": "fields = [StructField(\"id\", IntegerType(), True), StructField(\"rainfall\", DoubleType(), True)]\nschema = StructType(fields)\ndf = sqlContext.createDataFrame(rainfall, schema)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 4, "cell_type": "code", "source": "bucketizer = Bucketizer(splits=splits, inputCol=\"rainfall\", outputCol=\"discrainfall\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 5, "cell_type": "code", "source": "bucketedData = bucketizer.transform(df)\nbucketedData.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+--------+------------+\n| id|rainfall|discrainfall|\n+---+--------+------------+\n|  1|    10.2|         1.0|\n|  2|    17.1|         3.0|\n|  3|     9.6|         1.0|\n|  4|     5.0|         0.0|\n|  5|     3.4|         0.0|\n+---+--------+------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "from pyspark.mllib.feature import Normalizer\nfrom pyspark.mllib.linalg import Vectors", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 7, "cell_type": "code", "source": "data = [[1, 34.0, 587.0],[5, 76.0, 1005.0], [3, 22.0, 867.0], [5, 19.0, 475.0], [2, 22.0, 666.0]]\ninput = sc.parallelize(data)\ninput.take(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[[1, 34.0, 587.0], [5, 76.0, 1005.0], [3, 22.0, 867.0], [5, 19.0, 475.0], [2, 22.0, 666.0]]"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "from pyspark.mllib.feature import Normalizer\nfrom pyspark.mllib.linalg import Vectors\ndata = [[1, 34.0, 587.0],[5, 76.0, 1005.0], [3, 22.0, 867.0], [5, 19.0, 475.0], [2, 22.0, 666.0]]\ninput = sc.parallelize(data)\ninput.take(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[[1, 34.0, 587.0], [5, 76.0, 1005.0], [3, 22.0, 867.0], [5, 19.0, 475.0], [2, 22.0, 666.0]]"}], "metadata": {"collapsed": false}}, {"execution_count": 9, "cell_type": "code", "source": "normalizer = Normalizer()\nnormalized = normalizer.transform(input)\nnormalized.take(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[DenseVector([0.0017, 0.0578, 0.9983]), DenseVector([0.005, 0.0754, 0.9971]), DenseVector([0.0035, 0.0254, 0.9997]), DenseVector([0.0105, 0.04, 0.9991]), DenseVector([0.003, 0.033, 0.9995])]"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}